{
 "cells": [
  {
   "metadata": {
    "id": "fbc121e30a2defb3",
    "outputId": "60947ea4-f7da-4bdc-9c72-eb6e862a2d01",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "cell_type": "code",
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"rabieelkharoua/alzheimers-disease-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# https://www.kaggle.com/code/adhamtarek147/alzheimer-s-disease-prediction\n",
    "# Eksempel av løsning på datasettet"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(path + \"/alzheimers_disease_data.csv\")\n",
    "data.head()"
   ],
   "id": "98f2f58d45730b7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data.info()",
   "id": "78977d519517a585",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data.describe()",
   "id": "d7f7ea8eb496a36f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#som beskrevet i datasett sin beskrivelse  er doctorInCharge bare XXXconfid og dermed ikke relevant\n",
    "data_cleaned = data.drop(columns=[\"DoctorInCharge\"], axis=1)\n",
    "#vi trenger heller ikkje PatientID\n",
    "data_cleaned = data_cleaned.drop(columns=[\"PatientID\"], axis=1)\n",
    "data_cleaned.info()"
   ],
   "id": "9b022592d8f791d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Siden dette bare en enkel nettside så blir det vanskielig å få tilgang til medisisnke verdiser som CholesterolHDL og lignende.\n",
    "# Så dermed fjerner vi disse kolonnene siden vi ikkje kan forvente brukeren å kunne disse verdiene.\n",
    "data_cleaned = data_cleaned.drop(columns=[\"CholesterolTotal\",\"CholesterolLDL\",\"CholesterolHDL\",\"CholesterolTriglycerides\",\"MMSE\",\"FunctionalAssessment\",\"ADL\"], axis=1)\n",
    "data_cleaned.info()"
   ],
   "id": "803f378d5df163a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#",
   "id": "12813f70b83eade4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for column in data_cleaned.columns:\n",
    "    unique_values = data_cleaned[column].unique()\n",
    "    print(f\"Column '{column}' has {unique_values}.\")\n",
    "\n",
    "# vi ser her at et veldig stor foskjell mellom skaleringen til nokon av verdiene med, tall fra 0-3 og nokon fra 0-100\n",
    "# vi må normalisere de"
   ],
   "id": "6856f094a98814b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "columns_to_normalize = ['Age','BMI', 'AlcoholConsumption', 'PhysicalActivity', 'DietQuality', 'SleepQuality', 'SystolicBP','DiastolicBP']\n",
    "\n",
    "# skalere dataen\n",
    "standard_scaler = StandardScaler()\n",
    "data_cleaned[columns_to_normalize] = standard_scaler.fit_transform(data_cleaned[columns_to_normalize])\n",
    "data_cleaned.head()"
   ],
   "id": "e8a7f520e5d7d707",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Lagd av Gemeni\n",
    "# Encode the target variable since models require numerical input\n",
    "le = LabelEncoder()\n",
    "data_cleaned['Diagnosis'] = le.fit_transform(data_cleaned['Diagnosis'])\n",
    "\n",
    "X = data_cleaned.drop(columns=[\"Diagnosis\"], axis=1)\n",
    "y = data_cleaned[\"Diagnosis\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n"
   ],
   "id": "9d630c4684d998a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T19:02:33.259793Z",
     "start_time": "2025-10-24T19:02:01.099948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# vi tester flere modeler samtidig for å finne den beste\n",
    "# vi tester da gjerne modeler som har en predict_proba metode siden alzheimers som kan være skadelig å gi konkrette spådommer om\n",
    "# så vil bare gi en sannsynlighet for å ha sjukdommen og en anbefaling om å ta vidare tester\n",
    "\n",
    "# Med å sjå på eksempel løsning på datasetet(https://www.kaggle.com/code/adhamtarek147/alzheimer-s-disease-prediction) så ser vi at DecisionTreeClassifier, RandomForestClassifier, XGBClassifier og CatBoostClassifier gir gode resultater\n",
    "# Dermed så tester vi disse modellene her for å se hvilken av de er den beste på vårt modifisert datasett\n",
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='mlogloss'),\n",
    "    \"CatBoost\": CatBoostClassifier(verbose=0)\n",
    "}\n",
    "\n",
    "# Gitt av https://www.kaggle.com/code/adhamtarek147/alzheimer-s-disease-prediction\n",
    "param_grids = {\n",
    "    'Decision Tree': {'max_depth': [3, 5, 7, 12, None]},\n",
    "    'Random Forest': {'n_estimators': [50, 100, 200], 'max_depth': [3, 5, 7, 12, None]},\n",
    "    'XGBoost': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1], 'max_depth': [3, 5, 7]},\n",
    "    'CatBoost': {'iterations': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1]}\n",
    "}\n",
    "\n",
    "# Skrevet av Gemeni\n",
    "for name, model in models.items():\n",
    "    # Use 'f1_weighted' for multiclass classification scoring\n",
    "    grid_search = GridSearchCV(model, param_grids[name], cv=5, scoring='f1_weighted')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Inverse transform predictions to get original labels for the report\n",
    "    y_test_labels = le.inverse_transform(y_test)\n",
    "    y_pred_labels = le.inverse_transform(y_pred)\n",
    "\n",
    "    report = classification_report(y_test_labels, y_pred_labels)\n",
    "    print(f'{name} Classification Report:\\n{report}\\nBest Parameters: {grid_search.best_params_}\\n')"
   ],
   "id": "ea9611b2221bb2e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.85      0.79       277\n",
      "           1       0.63      0.46      0.54       153\n",
      "\n",
      "    accuracy                           0.71       430\n",
      "   macro avg       0.69      0.66      0.66       430\n",
      "weighted avg       0.70      0.71      0.70       430\n",
      "\n",
      "Best Parameters: {'max_depth': 3}\n",
      "\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.88      0.80       277\n",
      "           1       0.67      0.44      0.53       153\n",
      "\n",
      "    accuracy                           0.72       430\n",
      "   macro avg       0.70      0.66      0.67       430\n",
      "weighted avg       0.71      0.72      0.71       430\n",
      "\n",
      "Best Parameters: {'max_depth': 12, 'n_estimators': 200}\n",
      "\n",
      "XGBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.86      0.80       277\n",
      "           1       0.64      0.46      0.54       153\n",
      "\n",
      "    accuracy                           0.72       430\n",
      "   macro avg       0.69      0.66      0.67       430\n",
      "weighted avg       0.71      0.72      0.70       430\n",
      "\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}\n",
      "\n",
      "CatBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.86      0.81       277\n",
      "           1       0.67      0.52      0.59       153\n",
      "\n",
      "    accuracy                           0.74       430\n",
      "   macro avg       0.72      0.69      0.70       430\n",
      "weighted avg       0.73      0.74      0.73       430\n",
      "\n",
      "Best Parameters: {'iterations': 50, 'learning_rate': 0.01}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-24T19:01:47.932732Z",
     "start_time": "2025-10-24T19:01:41.715778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Vi ser her at det var en stor forskjell i restultatene som vi fikk fra eksempel løsningen på datasettet og det vi fikk her\n",
    "# Dermed så kan det være lurt å prøve de andre modellene også som er nevnt i eksempel løsningen på datasettet\n",
    "# siden det kan hende at de er bedre egnet for dette modifiserte datasettet\n",
    "\n",
    "models = {\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "}\n",
    "\n",
    "# Gitt av https://www.kaggle.com/code/adhamtarek147/alzheimer-s-disease-prediction\n",
    "param_grids = {\n",
    "    'K-Nearest Neighbors': {'n_neighbors': [3, 5, 7]},\n",
    "    'Logistic Regression': {'C': [0.1, 1, 10]},\n",
    "    'Support Vector Machine': {'C': [0.1, 1, 10], 'gamma': [0.1, 1, 'scale', 'auto']},\n",
    "}\n",
    "\n",
    "# Skrevet av Gemeni\n",
    "for name, model in models.items():\n",
    "    # Use 'f1_weighted' for multiclass classification scoring\n",
    "    grid_search = GridSearchCV(model, param_grids[name], cv=5, scoring='f1_weighted')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # Inverse transform predictions to get original labels for the report\n",
    "    y_test_labels = le.inverse_transform(y_test)\n",
    "    y_pred_labels = le.inverse_transform(y_pred)\n",
    "\n",
    "    report = classification_report(y_test_labels, y_pred_labels)\n",
    "    print(f'{name} Classification Report:\\n{report}\\nBest Parameters: {grid_search.best_params_}\\n')"
   ],
   "id": "949de0af553f0df1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.89      0.80       277\n",
      "           1       0.67      0.41      0.50       153\n",
      "\n",
      "    accuracy                           0.72       430\n",
      "   macro avg       0.70      0.65      0.65       430\n",
      "weighted avg       0.71      0.72      0.70       430\n",
      "\n",
      "Best Parameters: {'C': 1, 'gamma': 'auto'}\n",
      "\n",
      "K-Nearest Neighbors Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.77      0.71       277\n",
      "           1       0.39      0.26      0.31       153\n",
      "\n",
      "    accuracy                           0.59       430\n",
      "   macro avg       0.52      0.52      0.51       430\n",
      "weighted avg       0.56      0.59      0.57       430\n",
      "\n",
      "Best Parameters: {'n_neighbors': 3}\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.85      0.80       277\n",
      "           1       0.66      0.52      0.58       153\n",
      "\n",
      "    accuracy                           0.73       430\n",
      "   macro avg       0.71      0.68      0.69       430\n",
      "weighted avg       0.72      0.73      0.72       430\n",
      "\n",
      "Best Parameters: {'C': 10}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Her såg vi at faktisk så var Logistic Regression den beste modellen for dette modifiserte datasettet\n",
    "# med en vektet f1-score på 0.72\n",
    "# Dermed så bruker vi denne modellen videre\n",
    "\n"
   ],
   "id": "962ce08094183d83"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
